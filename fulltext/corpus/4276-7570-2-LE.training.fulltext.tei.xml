<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
		<p>According to the Liaison Committee on Medical Education (LCME), the nationally recognized accrediting
			<lb/>
			authority for allopathic medical schools in USA and Canada, &quot;there must be ongoing assessment that
			assures
			<lb/>
			students have acquired and can demonstrate on direct observation the core clinical skills, behaviors, and
			attitudes
			<lb/>
			that have been specified in the school&apos;s educational objectives.&quot; In this context, the LCME
			considers meeting this
			<lb/>
			standard to be absolutely necessary for accreditation.<ref type="biblio">[1]</ref> Therefore, effective
			methods for accomplishing direct
			<lb/>
			observation and assessment linked to educational objectives are vital for training medical students.
			However, several
			<lb/>
			challenges exist in the direct observation and assessment of a medical student&apos;s clinical skills. A
			major challenge to
			<lb/>
			overcome is that such skills are rarely observed. According to Howley, up to 81% of students reported never
			being
			<lb/>
			observed interviewing or examining a patient during core third year clerkships. <ref type="biblio">[2]</ref> Pulito
			reported that over of the
			<lb/>
			1056 comments generated by faculty on clinical evaluation forms, not a single one commented on the
			history-taking
			<lb/>
			or physical examination skills of medical students. He determined that evaluation of a student&apos;s
			clinical skills by
			<lb/>
			faculty is commonly inferred from other factors, such as case presentations, or derived from information
			received
			<lb/>
			from residents.
			<ref type="biblio">[3]
				<lb/>
			</ref>
		
		</p>
		<p>When direct observation does occur, standards for judging clinical performance are commonly not made
			<lb/>
			explicit to the evaluator. <ref type="biblio">[4]</ref> The lack of explicit performance standards affects
			the reliability and validity of
			<lb/>
			competency assessments. For example, in a study of 326 British medical students, Hill noted that up to 29%
			of a
			<lb/>
			student&apos;s score on a mini-CEX evaluation could be explained by faculty specific variables (i.e. the
			examiner&apos;s
			<lb/>
			stringency, subjectivity or satisfaction) -a higher percentage than that attributable to the student&apos;s
			actual ability.
			<ref type="biblio">[5]
				<lb/>
			</ref>
		</p>
		
		<p>Although standardized patient examinations are commonly employed in the assessment of clinical skills,
			<lb/>
			Holmboe has persuasively argued for continued importance of the role of faculty in assessing a trainee&apos;s
			<lb/>
			performance when caring for actual patients in a clinical setting.
			<ref type="biblio">[6]
				<lb/>
			</ref>
		</p>
		
		<p>The lack of effective strategies for making direct observation feasible in a real life clinical setting is
			the most
			<lb/>
			frequently cited barrier to allowing direct observation to occur.<ref type="biblio">[7] [2, 4, 8]</ref> Assessment
			tools that are flexible
			<lb/>
			enough to be used in dynamic clinical environments, and which have the capability to reliably monitor and
			record a
			<lb/>
			student&apos;s competencies as part of routine clinical workflow, have become key educational needs. To
			address some
			<lb/>
			of these prevailing concerns and shortcomings, we set out to assess the feasibility of an electronic problem
			specific
			<lb/>
			assessment tool (eCEX) in monitoring and recording student-competencies in our internal medicine clerkship.
			In
			<lb/>
			addition, we wanted to assess the impact of a grading incentive on the number of directly observed
			student-patient
			<lb/>
			encounters. We hypothesized that the eCEX would be useful in recording a student&apos;s clinical performance
			and that
			<lb/>
			students and evaluators would identify it as educationally valuable. Our secondary hypothesis was that a
			grading
			<lb/>
			incentive (defined below) would be associated with an increase in the number of directly observed
			student-patient
			<lb/>
			interactions.
			<lb/>
		</p>
		
		<head>Methods
			<lb/>
		</head>
		
		<p>Description of the technology -We developed a web-based content management system enabling the delivery
			<lb/>
			of content to mobile devices. The main purpose of creating this system was to enable users with average
			computing
			<lb/>
			skills to build flexible and customizable computer-based learning packages for distribution to mobile
			devices via
			<lb/>
			direct internet links or as a downloaded program. Users of the system can create and manage the structure
			and layout
			<lb/>
			of the content, links to other content within the program, and links to multimedia and external programs.
			This
			<lb/>
			content management system differs from computer programming languages in that extensive technical expertise
			is
			<lb/>
			not a prerequisite to producing web content. It is designed to make content distribution easy, so that it
			can be viewed
			<lb/>
			and interacted with using Microsoft Windows-based mobile devices and, more recently, the iPhone and iPod
			Touch.
			<lb/>
			Because the system has all its data stored in a central location, updating and viewing content in real-time
			is possible.
			<lb/>
			The system was built on the Microsoft.net TM framework, and because it is Web-based, it is widely
			accessible. We
			<lb/>
			have previously reported on our experience using this tool in distributing 19 of the core training problems
			from the
			<lb/>
			Clerkship Directors in Internal Medicine (CDIM) Curriculum Guide and on our experience with using this tool
			as a
			<lb/>
			patient-encounter log.
			<ref type="biblio">[9, 10]
				<lb/>
			</ref>
		</p>
		
		<p>We recently equipped this software with a tool for the development of real-time assessment of student-
			<lb/>
			competencies related to these 19 CDIM training problems. We built the assessment instrument using a
			cascading
			<lb/>
			tree structure. After adding the training problem (e.g. abdominal pain) in the root directory, we added the
			<lb/>
			competencies to be assessed (e.g. communication skills, history taking and physical exam skills) in
			subdirectories.
			<lb/>
			Within each of the specific competency directories, we added line items (e.g. for abdominal pain history
			taking we
			<lb/>
			added lines for pain characteristics, associated symptoms, past history of GI disease, family history of GI
			disease,
			<lb/>
			medications, social history). Within each line, using an HTML editor, we added the specific items to be
			assessed
			<lb/>
			(e.g. for abdominal pain → history taking → associated symptoms, added items included weight change, fever,
			<lb/>
			dysuria and GI related symptoms such as diarrhea, melena, etc, mostly derived from the CDIM Curriculum).
			When
			<lb/>
			displayed on a mobile device, these items appear as checkboxes. The authoring tool allows the user to define
			the
			<lb/>
			minimum number of checked items that constitutes a &quot;well done&quot; versus &quot;needs improvement&quot;
			performance.
			<lb/>
			Additionally, the tool allows the user to define the minimum number of &quot;well done&quot; lines required
			to receive a &quot;well
			<lb/>
			done&quot; as opposed to &quot;needs improvement&quot;. This allows the grade to be rendered automatically.
			The faculty
			<lb/>
			evaluators, however, have the option to override the grade at their discretion.
			<lb/>
		</p>
		
		<p>We allowed the students to choose the problem-specific performance objectives on which they wished to be
			<lb/>
			assessed (e.g. the physical exam objectives in a patient with abdominal pain). They, then, displayed the
			line items
			<lb/>
			for that objective on their device and handed it to a faculty member (including residents). The faculty
			preceptors
			<lb/>
			subsequently used the checklist to assess the students interacting with a real patient. We have termed this
			electronic
			<lb/>
			problem-specific clinical evaluation tool the eCEX.
			<lb/>
		</p>
		
		<p>(<ref type="figure">Figure 1</ref> here).
			<lb/>
		</p>
		
		<p>After the evaluation, the faculty member electronically signs the assessment record and this record of the
			student&apos;s
			<lb/>
			performance is then stored to a central server for tracking purposes.
			<lb/>
		</p>
		
		<p>Setting -The College of Human Medicine at Michigan State University trains third-year medical students in 6
			<lb/>
			communities throughout Michigan. During the academic year 2008 -2009, we implemented the eCEX as a required
			<lb/>
			element of the internal medicine clerkship in a convenience sample of 3 of our 6 community training sites.
			(student
			<lb/>
			(n = 56) with the remaining 3 training sites serving as controls and not using the eCEX (student n = 56).
			<lb/>
		</p>
		
		<p>For the grading incentive, students using the eCEX were to arrange 10 eCEX evaluations with their clinical
			<lb/>
			preceptors as part of the requirements to pass the clerkship. Students chose both the specific competency
			(e.g.
			<lb/>
			abdominal exam in a patient with abdominal pain) and the specific resident or attending evaluator at their
			discretion.
			<lb/>
			They were also responsible for orienting and assisting the evaluator in the use of the eCEX software.
			Students at the
			<lb/>
			control sites were required to complete a single observed complete history and physical exam, on a patient
			chosen
			<lb/>
			for them by the evaluator, to pass the clerkship. All additional direct observations in the clerkship were
			<lb/>
			discretionary.
			<lb/>
		</p>
		
		<p>At the end of each 8-week clerkship rotation, all students and faculty were surveyed at the eCEX sites on the
			<lb/>
			educational utility and technical aspects of the eCEX.
			<lb/>
		</p>
		
		<p>(Appendix 1 and 2 here).
			<lb/>
		</p>
		
		<p>Additionally, students in both study groups were surveyed on the number of times they were directly observed
			by
			<lb/>
			either faculty or residents during their 8 weeks on the clerkship. All data were obtained using the online
			survey
			<lb/>
			service &quot;Survey Monkey&quot; (www.survemonkey.com).
			<lb/>
		</p>
		
		<p>The data were downloaded into a spreadsheet and imported into SPSS for statistical analysis. The number of
			<lb/>
			directly observed encounters was analyzed with a one-tailed t-test between the communities using the eCEX
			and
			<lb/>
			those not using the eCEX, with significance set at &lt; .05. All other data were analyzed with descriptive
			statistics.
			<lb/>
		</p>
		
		<p>The local Institutional Review Board approved this study.
			<lb/>
		</p>
		
		<head>Results
			<lb/>
		</head>
		
		<p>Student and Evaluator eCEX use -All 56 students in the eCEX group successfully uploaded 10 or more
			<lb/>
			completed directly observed encounters. One hundred and twenty-nine faculty evaluators completed at least
			one
			<lb/>
			eCEX. The average number of eCEX evaluations per faculty evaluator was 4.6 (range = 1 to 19). Twenty-seven
			<lb/>
			faculty evaluators (20.9%) participated in a single eCEX whereas the remainder performed two or more eCEX
			<lb/>
			evaluations.
			<lb/>
		</p>
		
		<p>(<ref type="table">Table 1</ref> here).
			<lb/>
		</p>
		
		<p>Each student was observed by an average of 3.6 to 4 separate faculty evaluators depending on the community
			(range
			<lb/>
			= 1 to 7 evaluators per student).
			<lb/>
		</p>
		
		<p>Survey Data on the educational utility and technical aspects of eCEX -Fifty-three of 56 (91%) eCEX
			<lb/>
			students and 71 of 125 faculty evaluators (55%) responded to an online survey of their perceptions about the
			<lb/>
			educational utility and technical aspects of the eCEX
			<lb/>
		</p>
		
		<p>(<ref type="table">Table 2</ref> here).
			<lb/>
		</p>
		
		<p>Students and faculty both agreed that the eCEX helped them understand the specific history and physical exam
			<lb/>
			competencies being assessed during the encounters (75.5 -88.4% agree/strongly agree). Faculty felt that the
			eCEX
			<lb/>
			improved their assessments (74.7%) and their ability to give feedback (88.7%). The average time spent by
			faculty in
			<lb/>
			evaluating a student was 13.3 minutes. Faculty spent an average of 6.3 minutes on learning to use the
			electronic
			<lb/>
			checklist. They did not perceive accommodating the eCEX into their workday as difficult (84.2%). However,
			only
			<lb/>
			11.6 -20.7% of the students agreed or strongly agreed that arranging for either residents or faculty to
			observe them
			<lb/>
			was easy.
			<lb/>
		</p>
		
		<p>Number of directly observed encounters -All 112 students (100%) reported the number of times they were
			<lb/>
			directly observed by a resident or attending. Students in the eCEX group identified residents as observers
			of their
			<lb/>
			focused history and focused physical examinations an average of 7.7 (+/-8.3) and 11.5 (+/-11.3) times
			respectively.
			<lb/>
			Corresponding numbers for the control group were 4.6 +/-3.9 (p&lt;0.05) and 6.9 +/-7.4 (p&lt;0.05).
			Remainder of the
			<lb/>
			various measures between the two groups was not statistically significant.
			<lb/>
		</p>
		
		<p>Evaluation of competencies chosen by students -Students were free to choose the specific training problem
			<lb/>
			from our pre-specified list of problems and the competency in which they would be evaluated. All the
			students
			<lb/>
			chose at least one physical exam competency on which to be evaluated, whereas 20% of the students chose not
			to be
			<lb/>
			evaluated on communication skills or history taking.
			<lb/>
		</p>
		
		<p>(<ref type="table">Table 3</ref> here)
			<lb/>
		</p>
		
		<head>Discussion
			<lb/>
		</head>
		
		<p>There are several unique aspects to our study. To our knowledge, this is the first electronic mobile
			assessment
			<lb/>
			tool linked to problem-specific performance objectives tied to a nationally recognized curriculum.
			Additionally, we
			<lb/>
			believe this is the first comparative study assessing an intervention aimed at increasing the number of
			directly
			<lb/>
			observed student-patient encounters.
			<lb/>
		</p>
		
		<p>A number of studies have noted the difficulty encountered by faculty members in identifying the specific
			<lb/>
			performance criteria upon which medical students should be evaluated. In a study of the implementation of a
			mini-<lb/>CEX with 326 medical students, Hill commented that some examiners in the study were unsure of what
			standard to
			<lb/>
			expect from medical students and problems existed &quot;in trying to ensure that everyone was working to the
			same or
			<lb/>
			similar standards.&quot;<ref type="biblio">[5]</ref> In another study of 396 mini-CEX assessments on 176
			students, Fernando concluded that
			<lb/>
			faculty evaluators were unsure of the level of performance expected of the learners. <ref type="biblio">
				[11]
			</ref> In a study of 172 family
			<lb/>
			medicine clerks, on the role of direct observation in clerkship ratings, Hasnain suggested that the high
			level of
			<lb/>
			faculty-rater disagreement concerning clinical performance of students might have been due &quot;to the fact
			that
			<lb/>
			standards for judging clinical competence were not explicit&quot;.
			<ref type="biblio">[4]
				<lb/>
			</ref>
		</p>
		
		<p>These findings should not be surprising given the nature of nationally recognized medical curricula. For
			<lb/>
			example, the CDIM curriculum used in this study specifies that students should be able to demonstrate the
			ability to
			<lb/>
			obtain at least 17 specific historical items, demonstrate at least 10 separate physical exam steps, and be
			able to
			<lb/>
			counsel and encourage patients about smoking cessation, peak flow monitoring, environmental issues and
			asthma
			<lb/>
			action plans when indicated. This volume of information makes knowing and remembering what a medical student
			<lb/>
			should be able to do at a particular point in time and with a particular patient in a specific clinical
			situation very
			<lb/>
			difficult. Therefore, assessment for attainment of the specific curricular objectives is difficult, if not
			impossible.
			<lb/>
		</p>
		
		<p>Our results demonstrate that delivery of expected performance criteria just before planned assessments using
			the
			<lb/>
			eCEX fostered an understanding among faculty evaluators and students of what to expect during the
			evaluation;
			<lb/>
			specifically 88% of faculty agreed that the eCEX helped them understand the physical exam and history
			<lb/>
			competencies that they needed to evaluate, and 74% of students agreed that the eCEX helped understand the
			specific
			<lb/>
			history and physical exam competencies that they needed to know and demonstrate. In addition, 75% to 89% of
			the
			<lb/>
			faculty respondents felt that the eCEX improved their assessments and their ability to give feedback.
			<lb/>
		</p>
		
		<p>Our second objective was to test whether or not a grading incentive increased the number of directly observed
			<lb/>
			patient. The use of a grading incentive to accomplish CEX evaluations is not a new concept. Kogan used it as
			an
			<lb/>
			incentive for students to return their paper based mini-CEX booklets that were handed out at the beginning
			of their
			<lb/>
			internal medicine clerkship. However, the incentive was not tied to the number of forms completed, unlike in
			our
			<lb/>
			study where a grade was given only if the student completed 10 evaluations <ref type="biblio">[8]</ref>. As
			in our study, the actual results of
			<lb/>
			the mini-CEX evaluations in Kogan&apos;s study were not tied to the final grade.
			<lb/>
		</p>
		
		<p>Using a handheld device-based mini-CEX, Torre required students to complete 2 electronic mini-CEX
			<lb/>
			evaluations during a 2-month clerkship and attained 100% compliance. The presence or absence of a specific
			<lb/>
			grading incentive was not mentioned in this study. Again, the results of the actual CEX evaluations did not
			impact a
			<lb/>
			student&apos;s final clerkship evaluation in Torre&apos;s study. Neither Kogan&apos;s, nor Torre&apos;s
			study used a comparison group to
			<lb/>
			assess the effect of their mini-CEX intervention on the number of directly observed encounters.
			<lb/>
		</p>
		
		<p>In a study of 173 medical students, Fernando noted that only 41% of students met the requirement of 3 mini-
			<lb/>CEX evaluations in their final year of medical school. Fernando speculated that since these assessments
			were
			<lb/>
			formative and did not count toward the students&apos; final grade, the perceived importance of the mini-CEX
			may have
			<lb/>
			been diminished <ref type="biblio">[11]</ref>.
			<lb/>
		</p>
		
		<p>It has been estimated that adequate reliability of the mini-CEX can be achieved with 8 to 15 encounters <ref
			type="biblio">[5]</ref>.
			<lb/>
			Therefore, it is educationally important to ensure that a minimum number of CEX encounters occur. We have
			<lb/>
			previously demonstrated that a grading incentive was associated with a very high rate of compliance with
			students
			<lb/>
			meeting patient-log requirements. <ref type="biblio">[10]</ref> In this study, we also demonstrated that a
			grading incentive was associated
			<lb/>
			with 100% compliance in all 56 students completing 10 eCEX evaluations.
			<lb/>
		</p>
		
		<p>In their study, Fernando, et al commented: &quot;Providing formal training (on the use of the mini-CEX) to
			all
			<lb/>
			clinical staff who are likely to encounter students, or who may be approached by students for assessment, is
			<lb/>
			probably unrealistic&quot;. <ref type="biblio">[11]</ref> The results of our study strongly suggest that at
			least when it comes to the faculty&apos;s
			<lb/>
			understanding of what to assess, formal training may not be necessary. Giving students the responsibility to
			orient
			<lb/>
			faculty to the process, and to provide faculty with a detailed electronic checklist just prior to the
			assessment, may be
			<lb/>
			an answer to the dilemma posed by Fernando.
			<lb/>
		</p>
		
		<p>Several limitations, however, need to be kept in mind when interpreting the findings of our study. First,
			this is a
			<lb/>
			single institution study and the results may not be generalized to other institutions. Second, the three
			intervention
			<lb/>
			sites in the study were assigned based upon convenience and not via randomization. It is possible that
			site-specific
			<lb/>
			characteristics, and not our intervention, may have explained the results. Third, we did not require our
			students to
			<lb/>
			compete a full CEX, just a focused history or focused aspect of the physical examination. Thus our results
			may not
			<lb/>
			be applied to situations in which a full CEX is required. Fourth, although we had an excellent survey
			response from
			<lb/>
			the students, only 55% of the faculty responded, and those who did not respond may have been less
			enthusiastic
			<lb/>
			about the value of the eCEX. Fifth, we did not study the reliability of faculty assessments using the eCEX.
			The use
			<lb/>
			of structured checklists in the assessment of clinical performance is known to at least double the accuracy
			of trainee
			<lb/>
			assessment by identifying more performance errors.<ref type="biblio">[12]</ref> However, this improved
			accuracy did not translate into
			<lb/>
			more negative global assessments. The global assessments in our study were generated automatically, and
			several of
			<lb/>
			the faculty commented that they appreciated this aspect of the eCEX. We aim to study the inter-rater
			reliability of
			<lb/>
			the eCEX over the next academic year. Sixth, perhaps our study confirms a self-evident outcome; that a
			grading
			<lb/>
			incentive provides a strong stimulus for students. Although the expectations were quite different for the 2
			groups,
			<lb/>
			students in each group met their respective requirement for directly observed encounters. Given that we
			assessed 2
			<lb/>
			separate interventions -a grading incentive and the eCEX tool -we cannot say with certainty which
			intervention
			<lb/>
			increased the number of directly observed encounters. It is quite possible that the use of the eCEX, and not
			the
			<lb/>
			grading incentive, stimulated more frequent direct observation. Finally, we do not know how allowing
			students to
			<lb/>
			select the patient and specific competency assessment in the eCEX group influenced the studies outcome.
			<lb/>
		</p>
		
		<p>We conclude that the use of a problem-specific electronic mobile checklist increased the students&apos; and
			<lb/>
			evaluators&apos; understanding of the specific objectives of the observation, and the evaluators&apos;
			self-reported ability to
			<lb/>
			give feedback and provide assessments. We further conclude that a grading incentive increased the number of
			times
			<lb/>
			a student reported being observed by a resident physician while performing a focused history and physical
			<lb/>
			examination. Future studies on the inter-rater reliability of the eCEX are planned.
			<lb/>
		</p>
		
		<figure type="table">Table 1. Student and faculty eCEX use data
			<lb/>
			Site 1
			<lb/>
			Site 2
			<lb/>
			Site 3
			<lb/>
			Average number of
			<lb/>
			evaluators per
			<lb/>
			student per
			<lb/>
			clerkship
			<lb/>
			3.7 (1 -6)
			<lb/>
			3.6 (2 -7)
			<lb/>
			4 (1 -7)
			<lb/>
			Total number of
			<lb/>
			evaluators per
			<lb/>
			community
			<lb/>
			46
			<lb/>
			47
			<lb/>
			36
			<lb/>
			Average number of
			<lb/>
			evaluations per
			<lb/>
			evaluator
			<lb/>
			3.5 (+ 3.6)
			<lb/>
			Range (1 -19)
			<lb/>
			4.9 (+) 3.7
			<lb/>
			Range (1-14)
			<lb/>
			5.6 (+) 5.2
			<lb/>
			Range (1-19)
			<lb/>
			% Faculty doing 1
			<lb/>
			CEX
			<lb/>
			25%
			<lb/>
			17%
			<lb/>
			19.4%
			<lb/>
			% Faculty doing &gt;<lb/>
			2 CEX
			<lb/>
			46%
			<lb/>
			62%
			<lb/>
			66%
			<lb/>
			50 th percentile
			<lb/>
			2
			<lb/>
			4
			<lb/>
			3
			<lb/>
			7
			<lb/>
		</figure>
		
		<figure type="box">
			Summer 2008 thru Feb 2009; 129 separate faculty provided at least one eCEX
			<lb/>
			evaluation.
			<lb/>
		</figure>
		
		<figure type="table">Table 2. Faculty and student survey on the utility of eCEX
			<lb/>
			Faculty (n=72)
			<lb/>
			Students (n=53)
			<lb/>
			Strongly agree
			<lb/>
			or agree (%)
			<lb/>
			Disagree or
			<lb/>
			strongly
			<lb/>
			disagree (%)
			<lb/>
			Strongly agree
			<lb/>
			or agree (%)
			<lb/>
			Disagree or
			<lb/>
			strongly disagree
			<lb/>
			(%)
			<lb/>
			Understand
			<lb/>
			specific PE and
			<lb/>
			HX competencies*
			<lb/>
			88.4
			<lb/>
			2.9
			<lb/>
			73.6
			<lb/>
			17.0
			<lb/>
			Technical
			<lb/>
			Problems
			<lb/>
			1.5
			<lb/>
			88.2
			<lb/>
			Difficulty fitting
			<lb/>
			into work day
			<lb/>
			8.6
			<lb/>
			84.2
			<lb/>
			It was easy getting
			<lb/>
			resident to observe
			<lb/>
			20.7
			<lb/>
			52.8
			<lb/>
			It was easy getting
			<lb/>
			faculty to observe
			<lb/>
			11.6
			<lb/>
			69.8
			<lb/>
			Improved my
			<lb/>
			assessments
			<lb/>
			74.7
			<lb/>
			5.6
			<lb/>
			Improved my
			<lb/>
			ability to give
			<lb/>
			feedback
			<lb/>
			88.7
			<lb/>
			9.9
			<lb/>
			Time it took me to
			<lb/>
			learn to use the
			<lb/>
			program (min)
			<lb/>
			6.3
			<lb/>
			Time evaluating
			<lb/>
			student (min)
			<lb/>
			13.3
			<lb/>
			Overall Usefulness
			<lb/>
			95.3
			<lb/>
			1.5
			<lb/>
			43.4
			<lb/>
			28.3
			<lb/>
		</figure>
		
		<figure type="box">Summer 2008 thru Feb 2009; Neutral or NA not counted; 56 students used the eCEX, 53
			<lb/>
			(91%) responded to the survey on the eCEX utility; 70 out of 129 (54%) faculty/residents
			<lb/>
			responded to the survey
			<lb/>
			 * = &quot;needing to know and demonstrate&quot; for students, and &quot;improved ability to
			<lb/>
			identify the specific competencies needing to be evaluated&quot; for faculty
			<lb/>
		</figure>
		
		<figure type="table">Table 3. eCEX student data
			<lb/>
			Communication
			<lb/>
			Skills
			<lb/>
			History
			<lb/>
			taking
			<lb/>
			Physical
			<lb/>
			examination
			<lb/>
			Average Number
			<lb/>
			CEX&apos;s per
			<lb/>
			competency
			<lb/>
			domain
			<lb/>
			2.1 (+/-1.8)
			<lb/>
			3.4 (+/-2.1)
			<lb/>
			5.6 (+/-2.7)
			<lb/>
			% Students having
			<lb/>
			NO evaluations in
			<lb/>
			the domain
			<lb/>
			20.7
			<lb/>
			21.1
			<lb/>
			0
			<lb/>
		</figure>
		
		<figure type="box">
			The average number of patients that students were directly observed
			<lb/>
			interacting with was 7.8 
			<lb/>
		</figure>
		
		<figure type="table">
			
			<lb/>
			
			<lb/>
			5.
			<lb/>
			Routine use of CEX during the medicine clerkship will help
			<lb/>
			me improve my assessment of medical students&apos;<lb/>
			competencies
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			6.
			<lb/>
			The use of CEX improved my ability to provide feedback to
			<lb/>
			the students whom I evaluated
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			7.
			<lb/>
			The CEX improved my ability to identify specific physical
			<lb/>
			exam competencies which I needed to evaluate
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			8.
			<lb/>
			The CEX improved my ability to identify specific history-<lb/>taking competencies I needed to evaluate
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			9.
			<lb/>
			I liked the idea of evaluating students by observing multiple
			<lb/>
			short encounters
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			10. I experienced significant technical problems using the CEX
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			11. I would recommend that faculty, not the student, specify the
			<lb/>
			specific required observations
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			Very useful
			<lb/>
			Useful
			<lb/>
			Neutral
			<lb/>
			Not useful
			<lb/>
			12. As an evaluator, I would rate the overall educational
			<lb/>
			usefulness of the CEX as:
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
		</figure>
		
		<figure type="table">Student survey questionnaire.
			<lb/>
			Very Important /
			<lb/>
			Strongly Agree
			<lb/>
			Important / Agree
			<lb/>
			Neutral
			<lb/>
			Unimportant / Disagree
			<lb/>
			Very unimportant /
			<lb/>
			Strongly Disagree
			<lb/>
			1.
			<lb/>
			The CEX improved my understanding of specific physical
			<lb/>
			exam competencies I needed to know and demonstrate
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			2.
			<lb/>
			The CEX improved my understanding of specific history-<lb/>taking competencies I needed to know and
			demonstrate
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			3. I liked the idea of requiring multiple small observations
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			4.
			<lb/>
			I spent more than 5 minutes in orienting preceptors to the use
			<lb/>
			of the CEX
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			5. Ten separate observations were too many
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			6.
			<lb/>
			The students should be allowed to choose the specific
			<lb/>
			required observations (e.g. abdominal pain history)
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			7.
			<lb/>
			The faculty preceptors should specify/select all of the specific
			<lb/>
			required observations
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			Very Easy
			<lb/>
			Easy
			<lb/>
			Neutral
			<lb/>
			Difficult
			<lb/>
			Very Difficult
			<lb/>
			8. How easy was it to schedule a resident to observe you?
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			9 How easy was it to schedule an attending to observe you?
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			10 How easy was the overall use of the CEX?
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
			
			<lb/>
		</figure>
		
		<p>The following open-ended questions were included in survey questionnaires for both the
			<lb/>
			preceptors (in parentheses) and the students:
			<lb/>
			During the medicine Clerkship,
			<lb/>
		</p>
		
		<list>
			
			<item>
				1. How many times were you directly observed performing (did you directly observe
				<lb/>
				a student performing) a focused physical exam on a patient?
				<lb/>
			</item>
			
			<item>
				2. How many times were you directly observed performing (did you directly observe
				<lb/>
				a student performing) a full physical exam on a patient?
				<lb/>
			</item>
			
			<item>
				3. How many times were you directly observed performing (did you directly observe
				<lb/>
				a student performing) a focused history on a patient?
				<lb/>
			</item>
			
			<item>
				4. How many times were you directly observed performing (did you directly observe
				<lb/>
				a student performing) a full history on a patient?
				<lb/>
			</item>
			
			<item>
				5. How many times were you directly observed (did you directly observe a student)
				<lb/>
				counseling a patient?
				<lb/>
			</item>
		</list>
		
		<figure>Figure 1
			<lb/>
		</figure>
		
		<p>Some screen shots of the eCEX evaluation tool</p>
	
	
	</text>
</tei>
